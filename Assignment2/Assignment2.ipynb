{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MoQeEsZvHvvi"
   },
   "outputs": [],
   "source": [
    "# Norvig's spell checker\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]).union(known(edits1(word))) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import n-gram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1020385it [00:01, 606366.04it/s]\n",
      "1044268it [00:07, 133530.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "fourgram = {}\n",
    "fivegram = {}\n",
    "\n",
    "with open('bigrams.txt', encoding='latin-1') as f:\n",
    "    for line in tqdm(f):\n",
    "        list = line.split()\n",
    "        # update the bigram dictionary with the bigram and its count\n",
    "        if tuple(list[1:]) not in bigram:\n",
    "            bigram[tuple(list[1:])] = int(list[0])\n",
    "        else:\n",
    "            bigram[tuple(list[1:])] += int(list[0])\n",
    "\n",
    "with open('fivegrams.txt', encoding='latin-1') as f:\n",
    "    for line in tqdm(f):\n",
    "        list = line.split()\n",
    "        # update the fivegram dictionary with the fivegram and its count\n",
    "        if tuple(list[1:]) not in fivegram:\n",
    "            fivegram[tuple(list[1:])] = int(list[0])\n",
    "        else:\n",
    "            fivegram[tuple(list[1:])] += int(list[0])\n",
    "\n",
    "        # update the 4-gram dictionary with the fourgram and its count\n",
    "        if tuple(list[1:5]) not in fourgram:\n",
    "            fourgram[tuple(list[1:5])] = int(list[0])\n",
    "        else:\n",
    "            fourgram[tuple(list[1:5])] += int(list[0])\n",
    "        \n",
    "        if tuple(list[2:]) not in fourgram:\n",
    "            fourgram[tuple(list[2:])] = int(list[0])\n",
    "        else:\n",
    "            fourgram[tuple(list[2:])] += int(list[0])\n",
    "        \n",
    "        # update the 3-gram dictionary with the trigram and its count\n",
    "        if tuple(list[1:4]) not in trigram:\n",
    "            trigram[tuple(list[1:4])] = int(list[0])\n",
    "        else:\n",
    "            trigram[tuple(list[1:4])] += int(list[0])\n",
    "        \n",
    "        if tuple(list[2:5]) not in trigram:\n",
    "            trigram[tuple(list[2:5])] = int(list[0])\n",
    "        else:\n",
    "            trigram[tuple(list[2:5])] += int(list[0])\n",
    "        \n",
    "        if tuple(list[3:]) not in trigram:\n",
    "            trigram[tuple(list[3:])] = int(list[0])\n",
    "        else:\n",
    "            trigram[tuple(list[3:])] += int(list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_in_ngram(term, text):\n",
    "    \"\"\"\n",
    "    finds occurrences of a term within a context window of varying sizes in a given text\n",
    "\n",
    "    args:\n",
    "        term (str): the term to search for\n",
    "        text (str): the text to search within\n",
    "\n",
    "    returns:\n",
    "        dict: a dictionary where keys represent the length of the context window\n",
    "              and values represent the count of occurrences of the term within that window\n",
    "    \"\"\"\n",
    "\n",
    "    # nested function to choose the appropriate n-gram based on length\n",
    "    def choose_ngram(length):\n",
    "        if length == 1:\n",
    "            return bigram\n",
    "        elif length == 2:\n",
    "            return trigram\n",
    "        elif length == 3:\n",
    "            return fourgram\n",
    "        return fivegram\n",
    "\n",
    "    # slice the last characters of the text\n",
    "    end_slice = text[-4:]\n",
    "    result = {}\n",
    "    \n",
    "    # iterate over possible ngram lengths\n",
    "    for length in range(1, len(end_slice)):\n",
    "        # choose the appropriate ngram\n",
    "        ngrams = choose_ngram(length)\n",
    "        \n",
    "        # slice the context from the end slice\n",
    "        context_slice = end_slice[-length:]\n",
    "        key = tuple(context_slice + [term])\n",
    "        \n",
    "        # if the ngram exists in dictionary\n",
    "        if key in ngrams:\n",
    "            # if found, add its count to the result dictionary\n",
    "            result[length] = result.get(length, 0) + ngrams[key]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correct(contexts):\n",
    "    \"\"\"\n",
    "    finds the index of the context with the maximum length and maximum count\n",
    "\n",
    "    args:\n",
    "        contexts (list): a list of tuples where each tuple contains a context dictionary and its corresponding value\n",
    "\n",
    "    returns:\n",
    "        tuple: a tuple containing the index of the context with the maximum length and maximum count, and its value\n",
    "    \"\"\"\n",
    "\n",
    "    max_context_index = 0\n",
    "    max_length = 0\n",
    "    max_count = 0\n",
    "    max_context = 0\n",
    "\n",
    "    # iterate through each context and its corresponding value in the list\n",
    "    for i, (context, value) in enumerate(contexts):\n",
    "        # check if the current context is not empty and if its length or count is greater than the maximum\n",
    "        if context and (len(context) > max_length or (len(context) == max_length and context[max(context.keys())] > max_count)):\n",
    "            # update the variables to reflect the new maximum context\n",
    "            max_context_index = i\n",
    "            max_length = len(context)\n",
    "            max_count = context[max(context.keys())]\n",
    "            max_context = value\n",
    "\n",
    "    return max_context_index, max_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def ngram_correction(word, context):\n",
    "    \"\"\"\n",
    "    this function performs correction on a given word within a context using n-gram models\n",
    "\n",
    "    parameters:\n",
    "    - word (str): The word to be corrected\n",
    "    - context (list): The context within which the word occurs\n",
    "    - verbose (bool): Flag to enable/disable verbose output\n",
    "\n",
    "    returns:\n",
    "    - corrected_word (str): the corrected version of the word\n",
    "    \"\"\"\n",
    "    # candidate corrections for the given word from Norvig's spell checker\n",
    "    cands = candidates(word)\n",
    "    # print(cands)\n",
    "\n",
    "    # list to store found contexts for each candidate correction\n",
    "    contexts = []\n",
    "\n",
    "    for c in cands:\n",
    "        # find occurrences of the candidate correction in n-grams within the context\n",
    "        contexts.append((find_word_in_ngram(c, context), c))\n",
    "\n",
    "    # if no contexts are found for any candidate correction, choose the most probable correction\n",
    "    no_context_found = all(len(context) == 0 for context, _ in contexts)\n",
    "\n",
    "    if no_context_found:  # return if no context is found\n",
    "        return max(cands, key=P)\n",
    "\n",
    "    return find_correct(contexts)[1] # 0th index is the index of the context, 1st index is the context itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "**Ngram Dataset Selection**:\n",
    "   The implementation utilizes n-gram datasets, including bigrams, trigrams, fourgrams, and fivegrams. Trigrams and fourgrams were constructed by using fivegrams dataset. These datasets capture the context of words within a given text corpus, allowing for context-aware spelling correction. The choice of using multiple n-gram datasets enables the correction algorithm to consider different levels of context, from immediate neighboring words to more distant word associations.\n",
    "\n",
    "**Edit Distance Weights**:\n",
    "   The weights for edit1 (one edit away), edit2 (two edits away), and absent words probabilities are implicitly handled by the underlying implementation of Norvig's spell checker. The `candidates()` function generates possible corrections based on these edit distances, with `known()` and `edits1()` functions filtering candidates based on their presence in the vocabulary or their edit distance of one from the original word. This approach allows for a balance between considering closely related words and exploring further alternatives.\n",
    "\n",
    "**Context Search**:\n",
    "   The `find_word_in_ngram()` function is responsible for finding occurrences of a term within a context window of varying sizes in a given text. This function iterates over possible n-gram lengths and searches for the term within the corresponding n-gram dataset. By considering different context window lengths, the algorithm captures both local and global context information, enabling more accurate spelling correction decisions.\n",
    "\n",
    "**Correction Selection**:\n",
    "   After finding contexts for each candidate correction, the algorithm selects the correction with the maximum length and maximum count within the context. This selection strategy aims to prioritize corrections that are supported by a larger and more frequent context, improving the overall accuracy of spelling correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def mutate_word(word):\n",
    "    \"\"\"\n",
    "    mutates a word by randomly inserting, deleting, or changing a letter\n",
    "\n",
    "    args:\n",
    "    word (str): the input word to be mutated\n",
    "\n",
    "    returns:\n",
    "    str: the mutated word\n",
    "    \"\"\"\n",
    "    mutation_type = random.choice([\"insert\", \"delete\", \"change\"])\n",
    "    index = random.randint(0, len(word) - 1)  # Adjusted to ensure index is within the word's length\n",
    "    \n",
    "    if mutation_type == \"insert\":\n",
    "        letter = random.choice(string.ascii_lowercase)\n",
    "        word = word[:index] + letter + word[index:]\n",
    "    elif mutation_type == \"delete\":\n",
    "        if len(word) > 1 and index > 0:  # Ensure index is at least 1 to avoid empty string after deletion\n",
    "            word = word[:index] + word[index+1:]\n",
    "    else:  # change\n",
    "        letter = random.choice(string.ascii_lowercase)\n",
    "        while word[index] == letter:\n",
    "            letter = random.choice(string.ascii_lowercase)\n",
    "        word = word[:index] + letter + word[index+1:]\n",
    "\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download kaggle dataset https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews?select=test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv('test.csv',)\n",
    "# test_data.columns=['id', 'title', 'text']\n",
    "# test_data = test_data.dropna()[\"text\"].tolist()[:10000]\n",
    "# np.savetxt('test.txt', test_data, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(test_data)):\n",
    "#     test_data[i] = test_data[i].split()\n",
    "#     test_data[i] = test_data[i][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     punctuation = string.punctuation\n",
    "#     cleaned_text = text.translate(str.maketrans('', '', punctuation))\n",
    "#     return cleaned_text\n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     for j in range(len(test_data[i])):\n",
    "#         test_data[i][j] = remove_punctuation(test_data[i][j])\n",
    "\n",
    "# np.savetxt('test.txt', test_data, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = open('test.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:49<00:00, 203.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.748\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "answers_ngram = []\n",
    "\n",
    "correct = 0\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    context = test_data[i]\n",
    "    word = context[-1].lower()\n",
    "    context = context[:-1]\n",
    "    incorrect = mutate_word(word).lower()\n",
    "    corrected_word = ngram_correction(incorrect, context)\n",
    "    if word == corrected_word:\n",
    "        correct += 1\n",
    "\n",
    "    answers_ngram.append({\"correct word\": word, \n",
    "                          \"incorrect word\": incorrect, \n",
    "                          \"corrected word\": corrected_word, \n",
    "                          \"context\": context,\n",
    "                          \"is correct\": word == corrected_word})\n",
    "\n",
    "accuracy = correct / len(test_data)\n",
    "print(f\"Accuracy: {accuracy}\") # accuracy of ngram algorithm\n",
    "with open(\"answer_ngram.json\", \"w\") as file:\n",
    "    json.dump(answers_ngram, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'correct word': 'have',\n",
       "  'incorrect word': 'havv',\n",
       "  'corrected word': 'have',\n",
       "  'context': ['Despite', 'the', 'fact', 'that', 'I'],\n",
       "  'is correct': True},\n",
       " {'correct word': 'jul',\n",
       "  'incorrect word': 'jl',\n",
       "  'corrected word': 'el',\n",
       "  'context': ['I', 'bought', 'this', 'charger', 'in'],\n",
       "  'is correct': False},\n",
       " {'correct word': 'their',\n",
       "  'incorrect word': 'theizr',\n",
       "  'corrected word': 'their',\n",
       "  'context': ['Check', 'out', 'Maha', \"Energy's\", 'website.'],\n",
       "  'is correct': True},\n",
       " {'correct word': 'the',\n",
       "  'incorrect word': 'tfhe',\n",
       "  'corrected word': 'the',\n",
       "  'context': ['Reviewed', 'quite', 'a', 'bit', 'of'],\n",
       "  'is correct': True},\n",
       " {'correct word': 'incorrect',\n",
       "  'incorrect word': 'ikcorrect',\n",
       "  'corrected word': 'incorrect',\n",
       "  'context': ['I', 'also', 'began', 'having', 'the'],\n",
       "  'is correct': True}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_ngram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:50<00:00, 198.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6576\n"
     ]
    }
   ],
   "source": [
    "answers_norvig = []\n",
    "\n",
    "correct = 0\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    context = test_data[i]\n",
    "    word = context[-1].lower()\n",
    "    context = context[:-1]\n",
    "    incorrect = mutate_word(word).lower()\n",
    "    corrected_word = correction(incorrect)\n",
    "    if word == corrected_word:\n",
    "        correct += 1\n",
    "    answers_norvig.append({\"correct word\": word, \n",
    "                            \"incorrect word\": incorrect, \n",
    "                            \"corrected word\": corrected_word, \n",
    "                            \"context\": context,\n",
    "                            \"is correct\": word == corrected_word})\n",
    "\n",
    "print(f\"Accuracy: {correct/len(test_data)}\") # accuracy of Norvig's algorithm\n",
    "\n",
    "with open(\"answer_norvig.json\", \"w\") as file:\n",
    "    json.dump(answers_norvig, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'correct word': 'have',\n",
       "  'incorrect word': 'hoave',\n",
       "  'corrected word': 'have',\n",
       "  'context': ['Despite', 'the', 'fact', 'that', 'I'],\n",
       "  'is correct': True},\n",
       " {'correct word': 'jul',\n",
       "  'incorrect word': 'jjul',\n",
       "  'corrected word': 'soul',\n",
       "  'context': ['I', 'bought', 'this', 'charger', 'in'],\n",
       "  'is correct': False},\n",
       " {'correct word': 'their',\n",
       "  'incorrect word': 'thir',\n",
       "  'corrected word': 'this',\n",
       "  'context': ['Check', 'out', 'Maha', 'Energys', 'website'],\n",
       "  'is correct': False},\n",
       " {'correct word': 'the',\n",
       "  'incorrect word': 'tce',\n",
       "  'corrected word': 'the',\n",
       "  'context': ['Reviewed', 'quite', 'a', 'bit', 'of'],\n",
       "  'is correct': True},\n",
       " {'correct word': 'incorrect',\n",
       "  'incorrect word': 'iecorrect',\n",
       "  'corrected word': 'incorrect',\n",
       "  'context': ['I', 'also', 'began', 'having', 'the'],\n",
       "  'is correct': True}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_norvig[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the number of correctly predicted corrections increased using context from ngrams. Moreover, ngram_corrector runs in about the same time (1 second faster), which is also one of the advantages of the algorithm I wrote. You can check the results in answers_ngram.json and answers_norvig.json. By looking at them, it is possible to determine that words are indeed better defined where context is important. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
